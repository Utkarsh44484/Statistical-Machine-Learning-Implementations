{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13b49ddb",
      "metadata": {
        "id": "13b49ddb"
      },
      "source": [
        "## Assignment-2 : Naive Bayes\n",
        "### Name: Utkarsh Sathawane\n",
        "### Roll: 25CS60R75\n",
        "### Section: B"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe1f3e4",
      "metadata": {
        "id": "efe1f3e4"
      },
      "source": [
        "## Set-up and Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a10fd81",
      "metadata": {
        "id": "8a10fd81"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn pandas numpy matplotlib\n",
        "!pip install seaborn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "!pip install ucimlrepo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8634ad2a-0246-458a-a270-f46831a72343",
      "metadata": {
        "id": "8634ad2a-0246-458a-a270-f46831a72343"
      },
      "source": [
        "## Spambase Dataset(Odd Roll Numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f778d84f-c3a6-449d-be6a-85348bb90ab4",
      "metadata": {
        "id": "f778d84f-c3a6-449d-be6a-85348bb90ab4"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "spambase = fetch_ucirepo(id=94)\n",
        "\n",
        "x = spambase.data.features\n",
        "y = spambase.data.targets\n",
        "\n",
        "print(x.head())\n",
        "print(y.head())\n",
        "allfeature=[]\n",
        "for i in x:\n",
        "  allfeature.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34fbe5d7-4f9c-4467-a26a-379fe0c34c6d",
      "metadata": {
        "id": "34fbe5d7-4f9c-4467-a26a-379fe0c34c6d"
      },
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A data cleaning function was defined to ensure all feature data was numeric and to handle any missing values. Upon execution, it was found that 0 rows were dropped due to missing or bad values. The cleaned dataset was then split into training and testing sets using an 80/20 ratio, resulting in 3680 training samples and 921 test samples. The split was stratified to maintain the original class distribution in both sets."
      ],
      "metadata": {
        "id": "8tCJV3SIRCfZ"
      },
      "id": "8tCJV3SIRCfZ"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(x, y, allfeature):\n",
        "    x = x.apply(pd.to_numeric, errors='coerce')\n",
        "    label_name = y.columns[0]\n",
        "    full_df = pd.concat([x, y], axis=1)\n",
        "    count = len(full_df)\n",
        "    full_df.dropna(inplace=True)\n",
        "    print(\"Dropped\", count - len(full_df), \"rows with missing/bad values.\")\n",
        "    X = full_df[allfeature].values\n",
        "    y = full_df[label_name].values.ravel()\n",
        "    return X, y\n",
        "\n",
        "X, y = clean_data(x, y, allfeature)\n"
      ],
      "metadata": {
        "id": "DCFR1lOOBkT2"
      },
      "id": "DCFR1lOOBkT2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69eaa8a2-6465-4634-85c5-bec785bac3fd",
      "metadata": {
        "id": "69eaa8a2-6465-4634-85c5-bec785bac3fd"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a18699c-1116-437a-83d6-cb86b7d48051",
      "metadata": {
        "id": "0a18699c-1116-437a-83d6-cb86b7d48051"
      },
      "source": [
        "## Visualize the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial data visualization was performed. This included plotting a correlation heatmap of the all  features to observe relationships between them, and a count plot of the target variable ('Class') to visualize the distribution of spam versus non-spam emails in the dataset."
      ],
      "metadata": {
        "id": "vhB3m6mJRJON"
      },
      "id": "vhB3m6mJRJON"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a927a28-b253-48e1-8d7d-606e8bebc5fa",
      "metadata": {
        "id": "8a927a28-b253-48e1-8d7d-606e8bebc5fa"
      },
      "outputs": [],
      "source": [
        "def plotfeature(X_train, y_train, allfeature):\n",
        "    plot_df = pd.DataFrame(X_train, columns=allfeature)\n",
        "    plot_df['label'] = y_train\n",
        "    for feature in allfeature:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        sns.histplot(data=plot_df, x=feature, hue='label', kde=True, stat=\"density\", common_norm=False)\n",
        "        plt.title('Distribution of \"' + feature + '\"')\n",
        "        plt.legend(title='Spam', labels=['1 (Spam)', '0 (Not Spam)'])\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "plotfeature(X_train, y_train, allfeature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "769fab01-c4e3-4686-aefe-f70439e6612d",
      "metadata": {
        "id": "769fab01-c4e3-4686-aefe-f70439e6612d"
      },
      "source": [
        "## Implement Naive Bayes from Scratch    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A NaiveBayesClassifier class was implemented from scratch using Gaussian Naive\n",
        "Bayes principles.\n",
        "● fit(X, y): This method calculates the mean, variance, and log-priors for each feature\n",
        "based on the classes (spam/non-spam). A smoothing parameter alpha (defaulting\n",
        "to 1e-5) is added to the variance calculation to prevent numerical instability from\n",
        "features with zero variance.\n",
        "● predict(X): This method calculates the log-likelihood of each sample belonging to\n",
        "each class using the Gaussian probability density formula. It combines these\n",
        "likelihoods with the log-priors to determine the posterior probability and predicts the\n",
        "class with the highest score.\n",
        "● Helper functions were also created: findmetrics to calculate accuracy, precision,\n",
        "recall, and F1-score, and calculate_manual_confusion_matrix to generate a\n",
        "2x2 confusion matrix"
      ],
      "metadata": {
        "id": "Qvw1Kt2KQSYQ"
      },
      "id": "Qvw1Kt2KQSYQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941b90e8-0c1a-46ef-8f85-a57eda06d3f9",
      "metadata": {
        "id": "941b90e8-0c1a-46ef-8f85-a57eda06d3f9"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self._classes = None\n",
        "        self._log_priors = {}\n",
        "        self._means = {}\n",
        "        self._vars = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n, m = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        for c in self._classes:\n",
        "            X_c = X[y == c]\n",
        "            self._log_priors[c] = np.log(len(X_c) / n)\n",
        "            self._means[c] = np.mean(X_c, axis=0)\n",
        "            self._vars[c] = np.var(X_c, axis=0) + self.alpha\n",
        "\n",
        "    def predict(self, x):\n",
        "        ypred = []\n",
        "        for s in x:\n",
        "            scores = []\n",
        "            for c in self._classes:\n",
        "                lp = self._log_priors[c]\n",
        "                mu = self._means[c]\n",
        "                var = self._vars[c]\n",
        "                num = -((s - mu) ** 2) / (2 * var)\n",
        "                den = 0.5 * np.log(2 * np.pi * var)\n",
        "                ll = num - den\n",
        "                tot = np.sum(ll)\n",
        "                score = lp + tot\n",
        "                scores.append(score)\n",
        "            ypred.append(self._classes[np.argmax(scores)])\n",
        "        return np.array(ypred)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "287f7c99",
      "metadata": {
        "id": "287f7c99"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def findmetrics(y_true, y_pred, label=1):\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    for i in range(len(y_true)):\n",
        "        if y_true[i] == label and y_pred[i] == label:\n",
        "            TP += 1\n",
        "        elif y_true[i] != label and y_pred[i] != label:\n",
        "            TN += 1\n",
        "        elif y_true[i] != label and y_pred[i] == label:\n",
        "            FP += 1\n",
        "        elif y_true[i] == label and y_pred[i] != label:\n",
        "            FN += 1\n",
        "    total = TP + TN + FP + FN\n",
        "    accuracy = (TP + TN) / total if total else 0\n",
        "    precision = TP / (TP + FP) if (TP + FP) else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "\n",
        "\n",
        "# acc, prec, rec, f1 = findmetrics(y_test, y_pred)\n",
        "# cm = calculate_manual_confusion_matrix(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "Deze5How9Rfk"
      },
      "id": "Deze5How9Rfk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_manual_confusion_matrix(y_true, y_pred, label=1):\n",
        "    TN = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    TP = 0\n",
        "    for i in range(len(y_true)):\n",
        "        if y_true[i] == label and y_pred[i] == label:\n",
        "            TP += 1\n",
        "        elif y_true[i] != label and y_pred[i] != label:\n",
        "            TN += 1\n",
        "        elif y_true[i] != label and y_pred[i] == label:\n",
        "            FP += 1\n",
        "        elif y_true[i] == label and y_pred[i] != label:\n",
        "            FN += 1\n",
        "    return np.array([[TN, FP], [FN, TP]])"
      ],
      "metadata": {
        "id": "7os_T3K59wkO"
      },
      "id": "7os_T3K59wkO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testmodel(alphalist, xtrain, xtest, ytrain, ytest):\n",
        "    res = []\n",
        "    for a in alphalist:\n",
        "        modle = NaiveBayesClassifier(alpha=a)\n",
        "        modle.fit(xtrain, ytrain)\n",
        "        yptrain = modle.predict(xtrain)\n",
        "        yptest = modle.predict(xtest)\n",
        "        tracc, trpre, trrec, trf1 = findmetrics(ytrain, yptrain)\n",
        "        teacc, tepre, terec, tef1 = findmetrics(ytest, yptest)\n",
        "        res.append({\n",
        "            'alpha': a,\n",
        "            'train_accuracy': tracc,\n",
        "            'test_accuracy': teacc,\n",
        "            'train_precision': trpre,\n",
        "            'test_precision': tepre,\n",
        "            'train_recall': trrec,\n",
        "            'test_recall': terec,\n",
        "            'train_f1': trf1,\n",
        "            'test_f1': tef1\n",
        "        })\n",
        "        print(\"Alpha used :\", a, \"| test accuracy:\", round(teacc, 4), \"| test f1:\", round(tef1, 4))\n",
        "    return pd.DataFrame(res)\n",
        "\n",
        "\n",
        "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "res_df = testmodel(alpha, X_train, X_test, y_train, y_test)\n",
        "print(\"\\nhyperparameter tuning complete.\")"
      ],
      "metadata": {
        "id": "Qfx_3K2I9svr"
      },
      "id": "Qfx_3K2I9svr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res_df)\n"
      ],
      "metadata": {
        "id": "yvkMfx3heTxz"
      },
      "id": "yvkMfx3heTxz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "294ad85c-a754-41cd-9fab-fb9d327d2aa9",
      "metadata": {
        "id": "294ad85c-a754-41cd-9fab-fb9d327d2aa9"
      },
      "source": [
        "## Visualization and Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results: The model's performance was tracked across these alphas. The optimal performance was achieved with alpha = 0.001, yielding a test accuracy of 0.8339 and an F1-score of 0.8189.\n",
        "Analysis: Performance degraded significantly at higher alphas (1 and 10), which exhibited high precision but very low recall. The results, including accuracy, F1, precision, and recall plots versus alpha, and confusion matrices for the best model, were generated.\n"
      ],
      "metadata": {
        "id": "gMsAPQo7QcDn"
      },
      "id": "gMsAPQo7QcDn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e53ede",
      "metadata": {
        "id": "e9e53ede"
      },
      "outputs": [],
      "source": [
        "def trainbestmodel(results, xtrain, xtest, ytrain):\n",
        "    row = results.loc[results['test_accuracy'].idxmax()]\n",
        "    bestalpha = row['alpha']\n",
        "    print(\"\\nBest performing alpha (by test accuracy):\", bestalpha)\n",
        "    model = NaiveBayesClassifier(alpha=bestalpha)\n",
        "    model.fit(xtrain, ytrain)\n",
        "    ypredtrain = model.predict(xtrain)\n",
        "    ypredtest = model.predict(xtest)\n",
        "    return model, ypredtrain, ypredtest, bestalpha\n",
        "\n",
        "best_model, y_pred_train_best, y_pred_test_best, best_alpha = trainbestmodel(res_df, X_train, X_test, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01216c2a-3272-4ab5-9b97-ce88289e44d1",
      "metadata": {
        "id": "01216c2a-3272-4ab5-9b97-ce88289e44d1"
      },
      "outputs": [],
      "source": [
        "def plotconfmatrix(ytrain, ytest, ypredtrain, ypredtest, bestalpha):\n",
        "    cmtrain = calculate_manual_confusion_matrix(ytrain, ypredtrain)\n",
        "    cmtest = calculate_manual_confusion_matrix(ytest, ypredtest)\n",
        "    labels = ['Not Spam (0)', 'Spam (1)']\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(cmtrain, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title('Training set confusion matrix (alpha = ' + str(bestalpha) + ')')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.heatmap(cmtest, annot=True, fmt='d', cmap='Oranges', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plotconfmatrix(y_train, y_test, y_pred_train_best, y_pred_test_best, best_alpha)"
      ],
      "metadata": {
        "id": "FxjF0O9fdd-d"
      },
      "id": "FxjF0O9fdd-d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4cb660",
      "metadata": {
        "id": "dc4cb660"
      },
      "outputs": [],
      "source": [
        "# Build a plot with alpha along the x-axis and training/test accuracy\n",
        "def plotmetric(res_df, metric, title):\n",
        "    plt.plot(res_df['alpha'], res_df[f'train_{metric}'], 'o-', label='Train')\n",
        "    plt.plot(res_df['alpha'], res_df[f'test_{metric}'], 'o-', label='Test')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel(\"Smoothing parameter 'alpha' (log scale)\")\n",
        "    plt.ylabel(title)\n",
        "    plt.title(title + \" vs. smoothing parameter\")\n",
        "    plt.legend()\n",
        "\n",
        "def plotallmetrics(res_df):\n",
        "    metrics = [('accuracy', 'Accuracy'), ('precision', 'Precision'),\n",
        "               ('recall', 'Recall'), ('f1', 'F1-score')]\n",
        "    plt.figure(figsize=(16, 12))\n",
        "    for i, (metric, title) in enumerate(metrics, 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        plotmetric(res_df, metric, title)\n",
        "    plt.suptitle(\"Model performance vs. hyperparameter 'alpha'\", fontsize=18)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plotallmetrics(res_df)\n"
      ],
      "metadata": {
        "id": "nDNcDKd5egmk"
      },
      "id": "nDNcDKd5egmk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8ffce3a0",
      "metadata": {
        "id": "8ffce3a0"
      },
      "source": [
        "#### Explain which value of ‘a’ is most suitable for your dataset and why.\n",
        "#### Discuss how this smoothing parameter influences the model and helps prevent overfitting.\n",
        "-----------------------\n",
        "#### The best value for ‘a’ is 0.001 because it gives the highest test accuracy, meaning the model performs best on unseen data. In Gaussian Naive Bayes, this parameter works like a small adjustment to the variance (σ² + a) to avoid division-by-zero errors. It also helps reduce overfitting by slightly broadening the bell curve of each feature. This makes the model less overconfident on the training data and more capable of handling small changes in new or unseen emails.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227f04b1-3165-4737-a30d-cf3684b10c88",
      "metadata": {
        "id": "227f04b1-3165-4737-a30d-cf3684b10c88"
      },
      "source": [
        "## Investigating the Independence Assumption"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section tested the \"naive\" assumption of feature independence.\n",
        "A highly discriminative feature, 'char_freq_!' (index 51), was selected.\n",
        "This feature was duplicated one, two, three, and four times, increasing the dataset's feature count from 57 to 61.\n",
        "The model (using the best alpha of 0.001) was retrained and evaluated at each step.\n",
        "Findings: The test accuracy did not decrease; it remained stable at 0.8339 for 1 and 2 copies and slightly increased to 0.8350 for 3 and 4 copies.\n",
        "Analysis: This result demonstrates the classifier's naive independence assumption. Because the model treats all features as independent, adding correlated copies (duplicates) of a strong predictor (char_freq_!) causes the model to \"double-count\" (or triple, quadruple, etc.) its evidence. This squared the feature's contribution (or raised it to the 5th power with 4 copies), amplifying its importance rather than penalizing the model for redundancy."
      ],
      "metadata": {
        "id": "3a8XWFhcQw6X"
      },
      "id": "3a8XWFhcQw6X"
    },
    {
      "cell_type": "code",
      "source": [
        "# Select one feature that you believe is highly discriminative\n",
        "\n",
        "# Ans : char_freq_$ (index 51) is highly discriminative\n",
        "\n",
        "def selectfeature(X, allfeature, best_alpha):\n",
        "\n",
        "    feature_index_to_copy = 51\n",
        "    feature_name = allfeature[feature_index_to_copy]\n",
        "\n",
        "    print(f\"Investigating Independence Assumption \")\n",
        "    print(f\"Using best alpha: {best_alpha}\")\n",
        "    print(f\"Duplicating feature: '{feature_name}' (index {feature_index_to_copy})\")\n",
        "\n",
        "    col = X[:, [feature_index_to_copy]]\n",
        "\n",
        "    return col, feature_index_to_copy, feature_name\n",
        "\n",
        "featurecol, feature_index_to_copy, feature_name = selectfeature(X, allfeature, best_alpha)\n",
        "\n"
      ],
      "metadata": {
        "id": "08FOVGsVoo8s"
      },
      "id": "08FOVGsVoo8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dup_res = []\n",
        "xmod = X.copy()\n",
        "print(f\"Original dataset has {xmod.shape[1]} features.\")\n",
        "\n",
        "def evaluatemodel(xdata, ncopies):\n",
        "    xtrain, xtest, ytrain, ytest = train_test_split(xdata, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    model = NaiveBayesClassifier(alpha=best_alpha)\n",
        "    model.fit(xtrain, ytrain)\n",
        "    ypredtrain = model.predict(xtrain)\n",
        "    ypredtest = model.predict(xtest)\n",
        "    trainacc, trainpre, trainrec, trainf1 = findmetrics(ytrain, ypredtrain)\n",
        "    testacc, testpre, testrec, testf1 = findmetrics(ytest, ypredtest)\n",
        "    print(f\"  Trained with {ncopies} added copies | Total Features: {xdata.shape[1]:<3} | Test Acc: {testacc:.4f}\")\n",
        "    return {\n",
        "        'n_copies': ncopies,\n",
        "        'train_accuracy': trainacc, 'test_accuracy': testacc,\n",
        "        'train_precision': trainpre, 'test_precision': testpre,\n",
        "        'train_recall': trainrec, 'test_recall': testrec,\n",
        "        'train_f1': trainf1, 'test_f1': testf1\n",
        "    }\n",
        "\n",
        "res0 = evaluatemodel(xmod, 0)\n",
        "dup_res.append(res0)\n"
      ],
      "metadata": {
        "id": "99M9r7CAoic8"
      },
      "id": "99M9r7CAoic8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b835bf",
      "metadata": {
        "id": "23b835bf"
      },
      "outputs": [],
      "source": [
        "# Dataset 1(one copy)\n",
        "\n",
        "xmod = np.hstack((xmod, featurecol))\n",
        "res1 = evaluatemodel(xmod, 1)\n",
        "dup_res.append(res1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 2(two copies)\n",
        "\n",
        "xmod = np.hstack((xmod, featurecol))\n",
        "res2 = evaluatemodel(xmod, 2)\n",
        "dup_res.append(res2)"
      ],
      "metadata": {
        "id": "uNXHJKRPlRmh"
      },
      "id": "uNXHJKRPlRmh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461180b5",
      "metadata": {
        "id": "461180b5"
      },
      "outputs": [],
      "source": [
        "# Dataset 3(three copies)\n",
        "xmod = np.hstack((xmod, featurecol))\n",
        "res3 = evaluatemodel(xmod, 3)\n",
        "dup_res.append(res3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f32a0417",
      "metadata": {
        "id": "f32a0417"
      },
      "outputs": [],
      "source": [
        "# Dataset 4(four copies)\n",
        "xmod = np.hstack((xmod, featurecol))\n",
        "res4 = evaluatemodel(xmod, 4)\n",
        "dup_res.append(res4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dup_res_df = pd.DataFrame(dup_res)"
      ],
      "metadata": {
        "id": "N4fhcdrgADKW"
      },
      "id": "N4fhcdrgADKW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b4508fb0",
      "metadata": {
        "id": "b4508fb0"
      },
      "source": [
        "## Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60ceef3",
      "metadata": {
        "id": "b60ceef3"
      },
      "outputs": [],
      "source": [
        "metrics_to_plot = [('accuracy', 'Accuracy'), ('f1', 'F1-Score')]\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "for i, (metric, title) in enumerate(metrics_to_plot, 1):\n",
        "    plt.subplot(1, 2, i)\n",
        "    plt.plot(dup_res_df['n_copies'], dup_res_df[f'train_{metric}'], 'o-', label=f'Train {title}')\n",
        "    plt.plot(dup_res_df['n_copies'], dup_res_df[f'test_{metric}'], 'o-', label=f'Test {title}')\n",
        "    plt.xlabel(f\"Number of Added Copies of '{feature_name}'\")\n",
        "    plt.ylabel(title)\n",
        "    plt.title(f'{title} vs. Duplicated Features')\n",
        "    plt.xticks(range(5))\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "plt.suptitle(\"Effect of Violating the Independence Assumption\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72e22d4b-0fa0-4411-88e5-09f3037a6695",
      "metadata": {
        "id": "72e22d4b-0fa0-4411-88e5-09f3037a6695"
      },
      "source": [
        "## Discussions\n",
        "#### How does adding duplicate (and thus perfectly correlated) features affect the classifier's performance?\n",
        "#### Explain this behavior by referencing the Naive Bayes decision rule.\n",
        "#### What happens mathematically to the likelihood term when you add a copy of a feature?\n",
        "\n",
        "---------------------------\n",
        "1.\n",
        "Adding duplicate features generally degrades the classifier's test performance and increases overfitting. You'll see the test accuracy go down, while the train accuracy might stay high. The model becomes overconfident based on this one amplified feature and fails to generalize to new data.\n",
        "\n",
        "2.\n",
        "The Naive Bayes decision rule is y=argmaxy​P(y)∏P(xi​∣y). The core of this rule is the \"naive\" assumption that all features (xi​) are independent. By adding a duplicate feature, we perfectly violate this assumption. The classifier, being \"naive,\" doesn't know it's a copy and treats it as a new, independent piece of evidence. This \"double counting\" gives that one feature's \"vote\" an unfairly large influence on the final decision.\n",
        "\n",
        "3.\n",
        "When you add a copy xk​ of a feature xj​, the likelihood term in the product changes:\n",
        "\n",
        "    Original: ⋯×P(xj​∣y)×…\n",
        "\n",
        "    With 1 copy: ⋯×P(xj​∣y)×P(xk​∣y)×…\n",
        "\n",
        "Since xj​=xk​, their probabilities are identical, and the new term becomes: ⋯×(P(xj​∣y))2×…\n",
        "\n",
        "This squares the feature's contribution. If you add 4 copies, that single feature's probability is raised to the 5th power ((P(xj​∣y))5), massively amplifying its importance and drowning out the evidence from all other features."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wMdqnAuRF3Bf"
      },
      "id": "wMdqnAuRF3Bf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}