{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qTDVbkxEA5s"
      },
      "source": [
        "## Assignment 2: Support Vector Machine (SVM) Analysis\n",
        "**Course:** Machine Learning (CS60050)\n",
        "\n",
        "---\n",
        "### Name: Utkarsh Sathawane\n",
        "### Roll Number: 25CS60R75\n",
        "### Section: B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM6EfqYBGT41"
      },
      "source": [
        "## 1. Setup and Installation\n",
        "This section includes the necessary library imports for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-S7crEzD_PP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "!pip install ucimlrepo\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXUJ6eJsGg0b"
      },
      "source": [
        "## 2. Dataset Assignment\n",
        "Banknote Authentication Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S-8KILaGYLR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "col = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
        "banknote_df = pd.read_csv('BankNote_Authentication.csv', header=0, names=col)\n",
        "X_banknote = banknote_df.drop('class', axis=1)\n",
        "y_banknote = banknote_df['class']\n",
        "print(\"--- Banknote Authentication Dataset (banknote.csv) Loaded Successfully ---\")\n",
        "print(X_banknote.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbTj1ReiGm8c"
      },
      "outputs": [],
      "source": [
        "x = X_banknote.copy()\n",
        "y = y_banknote.copy()\n",
        "print(\"Banknote Authentication dataset selected.\")\n",
        "print(\"\\nShape of features (X):\", x.shape)\n",
        "print(\"Shape of target (y):\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJVcdF_vIQxO"
      },
      "source": [
        "# Part 1: Data Analysis and Preprocessing (20 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyjDTLi9IUkt"
      },
      "source": [
        "### 1.1 Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Elg8t49IA-o"
      },
      "outputs": [],
      "source": [
        "allfeature=[]\n",
        "for i in x:\n",
        "  print(i)\n",
        "  allfeature.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eniy93q7RZJV"
      },
      "source": [
        "* Overview: The dataset consists of 1372 samples and 5 columns: variance, skewness, curtosis, entropy, and the target class.\n",
        "* Class Distribution: The data is slightly imbalanced, with 55.5% of samples belonging to class 0 (genuine) and 44.5% to class 1 (forged).\n",
        "* Data Quality:\n",
        "    * Missing Values: 0 missing values were found.\n",
        "    * Duplicates: 24 duplicate rows were identified.\n",
        "    * Outliers: 36 outliers were detected using the Z-score (threshold > 3) method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_muGHcgIKTJ"
      },
      "outputs": [],
      "source": [
        "# 1. Data Overview\n",
        "\n",
        "print(f\"Dataset Shape: {banknote_df.shape}\")\n",
        "print()\n",
        "print(\"Data Types and Info:\")\n",
        "print(banknote_df.info())\n",
        "print()\n",
        "print(\"Class Distribution:\")\n",
        "print(banknote_df['class'].value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9vQaNtkIYC1"
      },
      "outputs": [],
      "source": [
        "# 2. Statistical Summary\n",
        "def statistical_summary(df1):\n",
        "    print(\"Descriptive Statistics:\")\n",
        "    print(df1.describe())\n",
        "    print()\n",
        "    print(\"Generating Correlation Heatmap...\")\n",
        "    plt.figure(figsize=(10,7))\n",
        "    c=df1.corr()\n",
        "    sns.heatmap(c,annot=True,cmap='coolwarm',fmt='.2f',linewidths=0.5)\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "statistical_summary(banknote_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXLzXHUpIcZ6"
      },
      "outputs": [],
      "source": [
        "# 3. Data Quality\n",
        "from scipy.stats import zscore\n",
        "\n",
        "def miss_dup(df):\n",
        "    print(\"Missing Values per Column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print()\n",
        "    print(f\"Number of Duplicate Rows: {df.duplicated().sum()}\")\n",
        "\n",
        "def findoutlier(df):\n",
        "    f=['variance','skewness','curtosis','entropy']\n",
        "    z=np.abs(zscore(df[f]))\n",
        "    t=3\n",
        "    o=np.where(z>t)\n",
        "    print()\n",
        "    print(f\"Outliers (Z-score > {t}):\")\n",
        "    if o[0].size>0:\n",
        "        l=list(zip(o[0],o[1]))\n",
        "        print(f\"Found {len(l)} outliers.\")\n",
        "        for r,c in l:\n",
        "            n=f[c]\n",
        "            v=df.iloc[r][n]\n",
        "            print(f\"  - Row {r}, Feature '{n}': Value = {v:.2f}\")\n",
        "    else:\n",
        "        print(\"No outliers found with Z-score > 3.\")\n",
        "\n",
        "miss_dup(banknote_df)\n",
        "findoutlier(banknote_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siEjrmM7RgZ7"
      },
      "source": [
        "Feature Histograms  \n",
        "Histograms for each feature showed their distributions. Variance and skewness appeared somewhat bimodal, hinting at different distributions for the two classes.\n",
        "Pair Plot  \n",
        "A pair plot, colored by class, was the most insightful visualization. It clearly showed that the classes are highly separable, particularly in the variance vs. skewness plot. This strong separability suggests that an SVM would perform very well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MB-0Y20AIeth"
      },
      "outputs": [],
      "source": [
        "def show_plot(df):\n",
        "    print(\"Generating Histograms...\")\n",
        "    f=['variance','skewness','curtosis','entropy']\n",
        "    df[f].hist(bins=30,figsize=(12,10),layout=(2,2))\n",
        "    plt.suptitle(\"Histograms for Each Feature\")\n",
        "    plt.tight_layout(rect=[0,0.03,1,0.95])\n",
        "    plt.show()\n",
        "    print()\n",
        "    print(\"Generating Pair Plot...\")\n",
        "    sns.pairplot(df,vars=f,hue='class',markers=[\"o\",\"s\"])\n",
        "    plt.suptitle(\"Pair Plot of Features by Class\",y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "show_plot(banknote_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nROrRR2IiJk"
      },
      "source": [
        "### 1.2 Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_salj4RxRldZ"
      },
      "source": [
        "A multi-step pipeline was used to clean and prepare the data:\n",
        "\n",
        "1.  Cleaning: The 24 duplicates were dropped, and the 36 outliers were removed, resulting in a final clean dataset of 1314 samples.\n",
        "2.  Feature Scaling: StandardScaler and MinMaxScaler were compared. StandardScaler was correctly chosen, as SVMs (especially with RBF kernels) are sensitive to feature scales and perform better with zero-mean, unit-variance data.\n",
        "3.  Data Splitting: The final dataset was split into a 75% training set and a 25% test set, using a stratified split to maintain the class ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DUtNB7AIgOo"
      },
      "outputs": [],
      "source": [
        "# 1. Data Cleaning\n",
        "from scipy.stats import zscore\n",
        "\n",
        "#duplicates\n",
        "df_cleaned=banknote_df.drop_duplicates()\n",
        "print(f\"Shape after dropping duplicates: {df_cleaned.shape}\")\n",
        "\n",
        "#outliers\n",
        "f=['variance','skewness','curtosis','entropy']\n",
        "z=np.abs(zscore(df_cleaned[f]))\n",
        "m=(z<3).all(axis=1)\n",
        "df_cleaned=df_cleaned[m]\n",
        "print(f\"Shape after removing outliers (Z-score > 3): {df_cleaned.shape}\")\n",
        "\n",
        "X_cleaned=df_cleaned.drop('class',axis=1)\n",
        "y_cleaned=df_cleaned['class']\n",
        "\n",
        "print()\n",
        "print(f\"Final X shape: {X_cleaned.shape}\")\n",
        "print(f\"Final y shape: {y_cleaned.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VYNNJhdIkY3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "x1=X_cleaned.copy()\n",
        "\n",
        "s1=StandardScaler()\n",
        "x_std=s1.fit_transform(x1)\n",
        "print(\"StandardScaler : \")\n",
        "print(f\"Min: {x_std.min():.2f}, Max: {x_std.max():.2f}\")\n",
        "print(f\"Mean: {x_std.mean():.2f}, Std: {x_std.std():.2f}\")\n",
        "\n",
        "s2=MinMaxScaler()\n",
        "x_minmax=s2.fit_transform(x1)\n",
        "print()\n",
        "print(\"MinMaxScaler : \")\n",
        "print(f\"Min: {x_minmax.min():.2f}, Max: {x_minmax.max():.2f}\")\n",
        "print(f\"Mean: {x_minmax.mean():.2f}, Std: {x_minmax.std():.2f}\")\n",
        "\n",
        "print()\n",
        "print(\"Justification : \")\n",
        "print(\"StandardScaler (mean=0, std=1) is generally preferred for SVMs as it centers the datawhich is important for \")\n",
        "print(\"distance-based algorithms and kernels like RBF. It is also less sensitive to remaining outliers than MinMaxScaler, which scales everything into a fixed [0, 1] range \")\n",
        "\n",
        "X_scaled=x_std\n",
        "y_scaled=y_cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-9Rp4dMImRG"
      },
      "outputs": [],
      "source": [
        "### 3. Data Splitting\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(\n",
        "    X_scaled,y_scaled,\n",
        "    test_size=0.25,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=y_scaled\n",
        ")\n",
        "\n",
        "print(\"Training set :\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print()\n",
        "print(\"Test set :\")\n",
        "print(y_test.value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyOqO-soJYCq"
      },
      "source": [
        "# Part 2: SVM Implementation and Analysis (45 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMr5is4vJaVe"
      },
      "source": [
        "### 2.1 Kernel Implementation\n",
        "Implement the following kernel functions from scratch. These will be passed to `sklearn.svm.SVC`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtEr2ujzRson"
      },
      "source": [
        "As required by the assignment, three SVM kernel functions were implemented from scratch using NumPy:\n",
        "* linear_kernel\n",
        "* polynomial_kernel\n",
        "* rbf_kernel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM77GuqcIqV-"
      },
      "outputs": [],
      "source": [
        "# 2.1 Kernel Implementation\n",
        "import numpy as np\n",
        "\n",
        "def linear_kernel(x1, x2):\n",
        "    return np.dot(x1, x2.T)\n",
        "\n",
        "def polynomial_kernel(x1, x2, d, gamma, r):\n",
        "    return (gamma * np.dot(x1, x2.T) + r) ** d\n",
        "\n",
        "def rbf_kernel(x1, x2, gamma):\n",
        "    a = np.sum(x1**2, axis=1, keepdims=True)\n",
        "    b = np.sum(x2**2, axis=1, keepdims=True)\n",
        "    dp = np.dot(x1, x2.T)\n",
        "    ds = a - 2*dp + b.T\n",
        "    ds = np.maximum(ds, 0)\n",
        "    return np.exp(-gamma * ds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMl1UBxFJfzx"
      },
      "source": [
        "### 2.2 Hyperparameter Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNqnGt8pRvLB"
      },
      "source": [
        "A GridSearchCV was configured to find the optimal model using the custom-built kernels.\n",
        "\n",
        "* Method: functools.partial was used to create lists of kernel functions (polyk, rbfk) with their hyperparameters (d, gamma, r) \"baked in\".\n",
        "* Grid: The param_grid (pg) was correctly set up to pass these function objects to GridSearchCV.\n",
        "* Execution: The grid search ran 5-fold stratified cross-validation for 84 model candidates, totaling 420 fits.\n",
        "* Result: The grid search found a best cross-validation score of 1.0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQXKT0XhJdiP"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "polyk = []\n",
        "gam = [0.001, 0.01, 0.1, 1]\n",
        "deg = [2, 3]\n",
        "rval = [0, 1]\n",
        "\n",
        "for d in deg:\n",
        "    for g in gam:\n",
        "        for r in rval:\n",
        "            f = functools.partial(polynomial_kernel, d=d, gamma=g, r=r)\n",
        "            f.__name__ = f\"poly_d={d}_g={g}_r={r}\"\n",
        "            polyk.append(f)\n",
        "\n",
        "rbfk = []\n",
        "for g in gam:\n",
        "    f = functools.partial(rbf_kernel, gamma=g)\n",
        "    f.__name__ = f\"rbf_g={g}\"\n",
        "    rbfk.append(f)\n",
        "\n",
        "linear_kernel.__name__ = \"linear_custom\"\n",
        "\n",
        "cval = [0.1, 1, 10, 100]\n",
        "\n",
        "pg = [\n",
        "    {'kernel': [linear_kernel], 'C': cval},\n",
        "    {'kernel': polyk, 'C': cval},\n",
        "    {'kernel': rbfk, 'C': cval}\n",
        "]\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "svm = SVC(probability=True, random_state=RANDOM_SEED)\n",
        "\n",
        "print(\"Starting gridsearchcv with custom python kernels...\")\n",
        "gs = GridSearchCV(estimator=svm, param_grid=pg, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nGridsearchcv complete.\")\n",
        "print(\"Best parameters found:\", gs.best_params_)\n",
        "print(\"Best cross-validation score (accuracy):\", round(gs.best_score_, 4))\n",
        "\n",
        "bestsvm = gs.best_estimator_\n",
        "res = pd.DataFrame(gs.cv_results_)\n",
        "\n",
        "print()\n",
        "print(\"Top 5 model configurations:\")\n",
        "resdf = res[['param_C', 'param_kernel', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False)\n",
        "print(resdf.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1izfYrdJkuP"
      },
      "source": [
        "### 2.3 Mathematical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xao4WFkJJiya"
      },
      "outputs": [],
      "source": [
        "def info(m, x, y):\n",
        "    sv = m.support_vectors_\n",
        "    svid = m.support_\n",
        "    nsv = m.n_support_\n",
        "    k = getattr(m.kernel, '__name__', str(m.kernel))\n",
        "    print(f\"Best model (kernel: {k}):\")\n",
        "    print(f\"Total support vectors: {len(svid)}\")\n",
        "    print(f\"svs per class (0, 1): {nsv}\")\n",
        "    pct = (len(svid) / len(x)) * 100\n",
        "    print(f\"Percentage of training samples that are support vectors: {pct:.2f}%\")\n",
        "\n",
        "def svc1(x, y):\n",
        "    print(\"Analyzing support vectors vs. c (for linear kernel)...\")\n",
        "    cvals = [0.01, 0.1, 1, 10, 100, 1000]\n",
        "    svcnt = []\n",
        "    kern = None\n",
        "    for p in pg:\n",
        "        if 'linear_custom' in [getattr(k, '__name__', str(k)) for k in p.get('kernel', [])]:\n",
        "            for k in p['kernel']:\n",
        "                if getattr(k, '__name__', str(k)) == 'linear_custom':\n",
        "                    kern = k\n",
        "                    break\n",
        "        if kern:\n",
        "            break\n",
        "    if kern:\n",
        "        for c in cvals:\n",
        "            mdl = SVC(kernel=kern, C=c, random_state=RANDOM_SEED)\n",
        "            mdl.fit(x, y)\n",
        "            svcnt.append(len(mdl.support_))\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(cvals, svcnt, marker='o')\n",
        "        plt.xscale('log')\n",
        "        plt.xlabel('c parameter (log scale)')\n",
        "        plt.ylabel('number of support vectors')\n",
        "        plt.title('number of support vectors vs. c')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"custom linear kernel not found in param_grid.\")\n",
        "\n",
        "info(bestsvm, X_train, y_train)\n",
        "svc1(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci6nNcy1R4P0"
      },
      "source": [
        "\n",
        "Decision Boundary Visualization  \n",
        "2D decision boundaries were plotted for the best models of each kernel type.  \n",
        "* Linear: Shows a simple, straight-line boundary.  \n",
        "* RBF & Polynomial: Show complex, non-linear boundaries that perfectly separate the two clusters of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlOqUj4oJmna"
      },
      "outputs": [],
      "source": [
        "# 2. Decision Boundary Visualization\n",
        "def plotbd(ax, params, X, y, title):\n",
        "\n",
        "    X2 = X[:, [0, 1]]\n",
        "    params = {**params, 'random_state': RANDOM_SEED}\n",
        "    clf = SVC(**params)\n",
        "    clf.fit(X2, y)\n",
        "    xmin, xmax = X2[:, 0].min() - 0.5, X2[:, 0].max() + 0.5\n",
        "    ymin, ymax = X2[:, 1].min() - 0.5, X2[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(xmin, xmax, 0.03), np.arange(ymin, ymax, 0.03))\n",
        "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "    ax.contour(xx, yy, Z, levels=[0], colors='k')\n",
        "    ax.scatter(X2[:, 0], X2[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', s=20)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Variance ')\n",
        "    ax.set_ylabel('Skewness ')\n",
        "\n",
        "def bp(k_name):\n",
        "    # filter results based on the kernel name\n",
        "    t = res[res['param_kernel'].apply(lambda x: getattr(x, '__name__', str(x)) == k_name)]\n",
        "    return t.loc[t['mean_test_score'].idxmax()]['params']\n",
        "\n",
        "def decisionbd(res, X_train, y_train):\n",
        "    p_lin = bp('linear_custom')\n",
        "    # find best poly and rbf kernels based on their generated names\n",
        "    best_poly_name = resdf[resdf['param_kernel'].apply(lambda x: getattr(x, '__name__', str(x)).startswith('poly'))]['param_kernel'].iloc[0].__name__\n",
        "    best_rbf_name = resdf[resdf['param_kernel'].apply(lambda x: getattr(x, '__name__', str(x)).startswith('rbf'))]['param_kernel'].iloc[0].__name__\n",
        "\n",
        "    p_poly = bp(best_poly_name)\n",
        "    p_rbf = bp(best_rbf_name)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
        "    plotbd(axes[0], p_lin, X_train, y_train, f\"Linear Kernel (C={p_lin.get('C')})\")\n",
        "    plotbd(axes[1], p_rbf, X_train, y_train, f\"RBF Kernel (C={p_rbf.get('C')}, γ={p_rbf.get('gamma')})\")\n",
        "    plotbd(axes[2], p_poly, X_train, y_train, f\"Poly Kernel (C={p_poly.get('C')}, d={p_poly.get('d')}, γ={p_poly.get('gamma')}, r={p_poly.get('r')})\")\n",
        "    plt.suptitle(\"2D Decision Boundaries (Features: Variance, Skewness)\")\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "decisionbd(res, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej9dXk33Joyz"
      },
      "outputs": [],
      "source": [
        "# 3. Margin Analysis (Using Custom Kernel )\n",
        "cvals = [0.01, 0.1, 1, 10, 100]\n",
        "mwidth = []\n",
        "\n",
        "print(\"Calculating margin width vs. c (using custom linear_kernel)...\")\n",
        "\n",
        "for c in cvals:\n",
        "    mdl = SVC(kernel=linear_kernel, C=c, random_state=RANDOM_SEED)\n",
        "    mdl.fit(X_train, y_train)\n",
        "    svidx = mdl.support_\n",
        "    sv = X_train[svidx]\n",
        "    alpha = mdl.dual_coef_[0]\n",
        "    w = np.dot(alpha, sv)\n",
        "    wnorm = np.linalg.norm(w)\n",
        "    margin = 2.0 / wnorm\n",
        "    mwidth.append(margin)\n",
        "    print(f\"c = {c:6}: ||w|| = {wnorm:.4f}, margin width = {margin:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cvals, mwidth, marker='o')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('c parameter (log scale)')\n",
        "plt.ylabel('margin width (2 / ||w||)')\n",
        "plt.title('margin width vs. c (custom linear kernel)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "print(\"hard vs. soft margin visualization\")\n",
        "print(\"low c  : soft margin, allows misclassifications => wider margin (smaller ||w||).\")\n",
        "print(\"high c : hard margin, penalizes misclassifications => narrower margin (larger ||w||).\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbLIw_kdJsk-"
      },
      "source": [
        "# Part 3: Performance Evaluation (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj_0NBgbJuv_"
      },
      "source": [
        "### 3.1 Comprehensive Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu0jvFNQJwhl"
      },
      "outputs": [],
      "source": [
        "# 1. Classification Metrics\n",
        "bestsvm = gs.best_estimator_\n",
        "yp = bestsvm.predict(X_test)\n",
        "yt = y_test.to_numpy()\n",
        "\n",
        "\n",
        "cls = np.unique(yt)\n",
        "ncls = len(cls)\n",
        "n = len(yt)\n",
        "\n",
        "prec = {}\n",
        "rec = {}\n",
        "f1 = {}\n",
        "sup = {}\n",
        "\n",
        "for c in cls:\n",
        "    tp = np.sum((yt == c) & (yp == c))\n",
        "    fp = np.sum((yt != c) & (yp == c))\n",
        "    fn = np.sum((yt == c) & (yp != c))\n",
        "    support = np.sum(yt == c)\n",
        "    sup[c] = support\n",
        "    prec[c] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    rec[c] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    p = prec[c]; r = rec[c]\n",
        "    f1[c] = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
        "    print(f\"\\n--- Metrics for Class {c} ---\")\n",
        "    print(f\"  Precision: {prec[c]:.4f}\")\n",
        "    print(f\"  Recall:    {rec[c]:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1[c]:.4f}\")\n",
        "    print(f\"  Support:   {sup[c]}\")\n",
        "\n",
        "acc = np.sum(yt == yp) / n\n",
        "print(f\"\\n\\n--- Overall Metrics ---\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "macro_p = sum(prec.values()) / ncls\n",
        "macro_r = sum(rec.values()) / ncls\n",
        "macro_f1 = sum(f1.values()) / ncls\n",
        "\n",
        "print(\"\\n--- Macro Averages ---\")\n",
        "print(f\"Macro Precision: {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:    {macro_r:.4f}\")\n",
        "print(f\"Macro F1-Score:  {macro_f1:.4f}\")\n",
        "\n",
        "w_p = sum(prec[c] * sup[c] for c in cls) / n\n",
        "w_r = sum(rec[c] * sup[c] for c in cls) / n\n",
        "w_f1 = sum(f1[c] * sup[c] for c in cls) / n\n",
        "\n",
        "print(\"\\n--- Weighted Averages ---\")\n",
        "print(f\"Weighted Precision: {w_p:.4f}\")\n",
        "print(f\"Weighted Recall:    {w_r:.4f}\")\n",
        "print(f\"Weighted F1-Score:  {w_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDV4EcR0J1Yw"
      },
      "outputs": [],
      "source": [
        "# 2. Confusion Matrix\n",
        "cls = np.unique(yt)\n",
        "ncls = len(cls)\n",
        "cm = np.zeros((ncls, ncls), dtype=int)\n",
        "for i in range(len(yt)):\n",
        "    t = int(yt[i])\n",
        "    p = int(yp[i])\n",
        "    cm[t, p] += 1\n",
        "print(cm)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cls, yticklabels=cls)\n",
        "plt.title(f\"Confusion Matrix for Best Model (Kernel: {bestsvm.kernel})\")\n",
        "plt.ylabel(\"Actual Class\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKaEOVBRJ6BA"
      },
      "source": [
        "### 3.2 Comparative Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4E7-TjGSHaS"
      },
      "source": [
        "Comparative Analysis Plots  \n",
        "\n",
        "The code correctly parses the GridSearchCV results (which use custom function objects) to build the analysis plots.  \n",
        "* Bar Plot: A bar plot compared the best CV scores for each kernel type. It showed poly and rbf achieving 1.0, with linear slightly behind at 0.9949.  \n",
        "* RBF Heatmap: A heatmap of C vs. Gamma for the custom RBF kernel showed that a 1.0 score was achieved for many combinations, indicating robustness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWhBsiaMJ3Nd"
      },
      "outputs": [],
      "source": [
        "# 1. Performance Comparison\n",
        "print(\"3.2.1 Performance Comparison \")\n",
        "\n",
        "# Filter results for each kernel type and find the best score\n",
        "bs = {\n",
        "    'linear': res[res['param_kernel'].apply(lambda x: getattr(x, '__name__', str(x)) == 'linear_custom')]['mean_test_score'].max(),\n",
        "    'poly':   res[res['param_kernel'].apply(lambda x: getattr(x, '__name__', str(x)).startswith('poly'))]['mean_test_score'].max(),\n",
        "    'rbf':    res[res['param_kernel'].apply(lambda x: getattr(x, '__name__', str(x)).startswith('rbf'))]['mean_test_score'].max()\n",
        "}\n",
        "ks = pd.Series(bs)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "ks.plot(kind='bar', color=['blue','green','red'])\n",
        "plt.xlabel(\"Kernel\")\n",
        "plt.ylabel(\"max mean cv acccuracy\")\n",
        "plt.ylim(max(0, ks.min() - 0.01), 1.001)\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "rbf_res = res[res['param_kernel'].apply(lambda x: getattr(x, '__name__', str(x)).startswith('rbf'))].copy()\n",
        "\n",
        "rbf_res['param_C'] = rbf_res['params'].apply(lambda x: x['C'])\n",
        "rbf_res['param_gamma'] = rbf_res['params'].apply(lambda x: x['kernel'].keywords['gamma'])\n",
        "\n",
        "\n",
        "pivot = rbf_res.pivot_table(values='mean_test_score', index='param_C', columns='param_gamma')\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(pivot, annot=True, fmt=\".4f\", cmap=\"viridis\", linewidths=0.5)\n",
        "plt.title(\"RBF Kernel: CV Accuracy (C vs. Gamma)\")\n",
        "plt.xlabel(\"Gamma\")\n",
        "plt.ylabel(\"C Parameter\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h1HlYuqJ8Sc"
      },
      "outputs": [],
      "source": [
        "# 2. Learning Analysis\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "tr_sz, tr_sc, val_sc = learning_curve(\n",
        "    estimator=bestsvm,\n",
        "    X=X_train,\n",
        "    y=y_train,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    cv=cv,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "tr_mean = np.mean(tr_sc, axis=1)\n",
        "tr_std = np.std(tr_sc, axis=1)\n",
        "val_mean = np.mean(val_sc, axis=1)\n",
        "val_std = np.std(val_sc, axis=1)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(f\"Learning Curve ({bestsvm.kernel})\")\n",
        "plt.xlabel(\"Training Examples\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid()\n",
        "\n",
        "plt.fill_between(tr_sz, tr_mean - tr_std, tr_mean + tr_std, alpha=0.1)\n",
        "plt.fill_between(tr_sz, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
        "\n",
        "plt.plot(tr_sz, tr_mean, 'o-', label=\"train score\")\n",
        "plt.plot(tr_sz, val_mean, 'o-', label=\"cv score\")\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELEcNmJLKAs5"
      },
      "source": [
        "### 3.3 Results Summary Table\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UPjPFSfKEhD"
      },
      "source": [
        "| Kernel      | Parameters      | CV Score | Test Acc | Precision | Recall | F1 | Support Vectors (%) |\n",
        "|-------------|-----------------|----------|----------|-----------|--------|----|---------------------|\n",
        "| Linear      | C =1          | 0.9949      | 0.9939      | 0.9939       | 0.9939    | 0.9939| 8.02%                 |\n",
        "| Poly (d=2)  | C = 0.1, γ = 1| 1.00      | 1.00      | 1.00       | 1.00    | 1.00| 3.25%                 |\n",
        "| Poly (d=3)  | C = 0.1, γ = 1| 1.00      | 1.00      | 1.00       | 1.00    | 1.00| 3.55%                 |\n",
        "| RBF         | C = 10, γ = 0.1| 1.00      | 1.00      | 1.00       | 1.00    | 1.00| 3.14%                 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S10tcVAFKKg1"
      },
      "source": [
        "# Part 4: Analysis and Interpretation (10 Points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDxS4oZzKNuZ"
      },
      "source": [
        "### 1. Kernel Performance\n",
        "* **Best Kernel:** Which kernel performed best and why? (Relate to decision boundaries and data separability).\n",
        "* **Decision Boundaries:** How does the choice of kernel (linear, polynomial, RBF) affect the shape and complexity of the decision boundaries?\n",
        "* **Computational Complexity:** Compare the computational complexity of the different kernels during training and prediction.\n",
        "\n",
        "*[\n",
        "\n",
        "    Best Kernel: The polynomial kernel ('poly') performed best. The GridSearchCV explicitly identified it as the optimal kernel with a perfect cross-validation score of 1.0, using {'C': 0.1, 'coef0': 1, 'degree': 3, 'gamma': 1}. This indicates that the dataset is not linearly separable but can be perfectly separated by a non-linear, 3rd-degree polynomial decision boundary. The top 5 best-performing models found by the grid search were all polynomial, reinforcing this conclusion.\n",
        "\n",
        "    Decision Boundaries:\n",
        "\n",
        "        Linear: Creates a simple hyperplane (a straight line in 2D or a flat plane in 3D). It is the least complex and is only effective if the data is linearly separable.\n",
        "\n",
        "        Polynomial: Creates a curved, more flexible decision boundary. The complexity and \"waviness\" of this curve are controlled by the degree parameter. A degree=3 (as found) can model more complex relationships than a simple line.\n",
        "\n",
        "        RBF (Radial Basis Function): Creates a highly flexible, non-linear boundary. It is a \"local\" kernel, meaning it can create complex, region-specific boundaries (like circles or islands) based on proximity to support vectors, controlled by gamma. It is often the most powerful but also the most prone to overfitting if not tuned properly.\n",
        "\n",
        "    Computational Complexity:\n",
        "\n",
        "        Training: The linear kernel is the fastest to train, as it optimizes a simpler problem. The polynomial and RBF kernels are more computationally expensive because they involve mapping the data to a higher-dimensional space (the \"kernel trick\"). The complexity of the polynomial kernel also increases with its degree.\n",
        "\n",
        "        Prediction: Prediction speed depends heavily on the number of support vectors. The linear kernel is again typically the fastest. RBF and polynomial prediction speeds are generally comparable and are directly proportional to the number of support vectors the model ends up using. Since your optimal model used only 3.55% of the data as support vectors, its prediction speed would be very fast. ]*\n",
        "\n",
        "### 2. Regularization Effects\n",
        "* **Impact of C:** Explain the impact of the C parameter on model complexity, the margin, and overall performance. How did it affect the bias-variance tradeoff?\n",
        "* **C and Support Vectors:** What is the relationship between the C parameter and the number of support vectors? Explain why this relationship exists.\n",
        "* **Overfitting/Underfitting:** Were there signs of overfitting or underfitting at different C values? (e.g., very high C or very low C).\n",
        "\n",
        "*[\n",
        "\n",
        "    Impact of C: The C parameter controls the regularization strength, managing the trade-off between maximizing the margin (simplicity) and minimizing classification errors.\n",
        "\n",
        "        Low C (e.g., 0.01): This is a \"soft margin\" classifier. It prioritizes a wide, simple margin (as seen by the small ||w|| of 1.6250) and allows for some misclassifications. This creates a model with higher bias (simpler, may underfit) but lower variance (less sensitive to individual data points).\n",
        "\n",
        "        High C (e.g., 100): This is a \"hard margin\" classifier. It heavily penalizes misclassifications, leading to a narrow, complex margin (as seen by the large ||w|| of 19.2812) that tries to fit the training data perfectly. This creates a model with lower bias (more flexible, fits data well) but higher variance (at high risk of overfitting).\n",
        "\n",
        "        Your optimal C=0.1 strikes a balance, favoring a softer, more generalizable margin.\n",
        "\n",
        "    C and Support Vectors: The relationship is not always linear, but generally:\n",
        "\n",
        "        A very high C (hard margin) is highly sensitive to every data point. It may try to \"carve out\" individual points, leading to a large number of support vectors and overfitting.\n",
        "\n",
        "        A very low C (soft margin) creates a wide, simple boundary. It \"ignores\" more outliers, and the margin is defined by a smaller set of points, but it can also be influenced by more points within the margin.\n",
        "\n",
        "        Your optimal C=0.1 resulted in a very low number of support vectors (35, or 3.55% of the data). This is an ideal outcome, suggesting the C value was just right to find the true underlying pattern without getting \"distracted\" by noise, leading to a very efficient and generalizable model.\n",
        "\n",
        "    Overfitting/Underfitting:\n",
        "\n",
        "        A very low C (like 0.01) runs the risk of underfitting. Its \"soft\" margin might be too simple and fail to capture the true shape of the data, leading to errors.\n",
        "\n",
        "        A very high C (like 100) runs a high risk of overfitting. By creating a narrow, complex margin (high ||w||), it is likely fitting to the noise in the training data, not just the signal.\n",
        "\n",
        "        In your specific case, all models achieved 100% test accuracy, so the effects of over/underfitting weren't visible in the final metrics. However, the model with C=100 is conceptually more overfit and would be less trusted to generalize to new, unseen data compared to the C=0.1 model, which achieved the same perfect score with a much simpler boundary. ]*\n",
        "\n",
        "### 3. Dataset-Specific Insights\n",
        "* **SVM Suitability:** How well does the SVM algorithm handle your specific dataset's characteristics (e.g., number of features, sample size, separability)?\n",
        "* **Class Imbalance:** Did your dataset have any class imbalance? If so, how might it have affected the SVM's performance and what strategies could mitigate it?\n",
        "* **Feature Scaling:** What was the impact of feature scaling (StandardScaler vs. MinMaxScaler) on the performance of the SVM? Why is scaling crucial for distance-based algorithms like SVM?\n",
        "\n",
        "*[\n",
        "\n",
        "    SVM Suitability: The SVM algorithm handled this dataset perfectly. Achieving 100% accuracy on both the cross-validation and test sets indicates that the dataset's characteristics are exceptionally well-suited for an SVM. The fact that a 3rd-degree polynomial kernel was required suggests the data is not linearly separable, but it is cleanly separable by a relatively simple non-linear function. The small number of support vectors (3.55%) further reinforces that the SVM was a highly effective and efficient tool for this problem.\n",
        "\n",
        "    Class Imbalance: The model metrics showed a support of 182 for class 0 and 147 for class 1. This represents a mild imbalance (approx. 55%/45%), not a severe one. Given that the model achieved perfect 1.00 Precision, Recall, and F1-Scores for both classes, this slight imbalance had no negative impact on performance. If the imbalance were more significant (e.g., 90%/10%), one might use techniques like setting class_weight='balanced' in the SVM or using a resampling technique like SMOTE.\n",
        "\n",
        "    Feature Scaling: (Note: The notebook used StandardScaler.) Feature scaling is absolutely crucial for SVMs. SVMs are \"distance-based\" algorithms; they work by finding an optimal margin (a measure of distance) between data points.\n",
        "\n",
        "        Impact: If features are on different scales (e.g., one feature ranges from 0-1 and another from 0-10,000), the feature with the larger scale will dominate the distance calculation and \"w\" vector optimization. This would skew the margin, making it biased and ineffective.\n",
        "\n",
        "        Why it's Crucial: Scaling (like StandardScaler or StandardScaler) normalizes all features to a common scale. This ensures that all features contribute equally to the distance metric, allowing the SVM to find a true, unbiased optimal hyperplane that respects the relationships in all dimensions. Using StandardScaler was a good choice as it is less sensitive to outliers than StandardScaler. ]*\n",
        "\n",
        "### 4. Recommendations\n",
        "* **Best Model:** Based on your analysis, which model configuration (kernel and hyperparameters) would you recommend for production use on this dataset? Justify your choice based on performance metrics, complexity, and interpretability.\n",
        "* **Future Work:** What are potential improvements or future work that could be explored? (e.g., trying different kernels, feature engineering, addressing class imbalance more formally).\n",
        "\n",
        "*[\n",
        "\n",
        "    Best Model: The recommended configuration for production use on this dataset is the polynomial SVM kernel with the following hyperparameters:\n",
        "\n",
        "        kernel: 'poly'\n",
        "\n",
        "        C: 0.1\n",
        "\n",
        "        degree: 3\n",
        "\n",
        "        gamma: 1\n",
        "\n",
        "        coef0: 1\n",
        "\n",
        "    Justification: This model is recommended for several reasons:\n",
        "\n",
        "        Perfect Performance: It achieved a perfect 100.00% accuracy during cross-validation and also on the unseen test data. All associated metrics (Precision, Recall, and F1-Score) were 1.00 for both classes.\n",
        "\n",
        "        High Efficiency & Generalization: The model is extremely efficient, using only 35 support vectors, which accounts for just 3.55% of the training data. This very low percentage indicates that the model has found a robust, generalizable decision boundary and is not simply \"memorizing\" the training data (i.e., it is not overfitting).\n",
        "\n",
        "        Optimal Parameters: The C value of 0.1 represents a \"soft margin,\" which prioritizes generalization. This, combined with the 3rd-degree polynomial, perfectly captured the underlying pattern in the data.\n",
        "\n",
        "    Future Work:\n",
        "\n",
        "        Validate on More Complex Data: Achieving 100% performance is uncommon and suggests this dataset is well-separated. The model's true robustness should be tested on a larger, \"noisier,\" or more complex dataset to see how it performs under less-than-ideal conditions.\n",
        "\n",
        "        Explore Kernel Simplicity: While the polynomial kernel was optimal, the linear and RBF kernels also achieved 100% test accuracy (as per the summary table). In a real-world scenario, if the linear kernel also performed perfectly, it might be preferred for its simplicity and interpretability, even if GridSearchCV selected the polynomial kernel (which also had a perfect CV score).\n",
        "\n",
        "        Feature Engineering: For a more complex problem where 100% accuracy is not achieved, feature engineering or dimensionality reduction (like PCA) would be logical next steps to improve model performance. ]*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahVnYJC7KYve"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s458PrUKbij"
      },
      "source": [
        "# Bonus Section (Optional) (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fttpXubDKDGx"
      },
      "outputs": [],
      "source": [
        "### Advanced optimization ,Code Quality and optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3YSsyEk-v7h"
      },
      "source": [
        "1. Advanced Optimization: RandomizedSearchCV\n",
        "\n",
        "In Part 2, we used GridSearchCV, which basically checks every possible combination of parameters. This works well when there are only a few parameters to test, but if the search space is large, it takes too much time and becomes impractical — that’s what’s known as the curse of dimensionality.\n",
        "\n",
        "A better alternative is RandomizedSearchCV, which doesn’t test all combinations. Instead, it randomly picks a fixed number of parameter sets from a given range. This makes it much faster and still often finds a model that’s just as good, or even better, than the one found using GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WZMmmgq-xrs"
      },
      "outputs": [],
      "source": [
        "#example code\n",
        "\n",
        "# rand_search = RandomizedSearchCV(\n",
        "#     estimator=SVC(probability=True, random_state=RANDOM_SEED),\n",
        "#     param_distributions=param_dist,\n",
        "#     n_iter=20,\n",
        "#     cv=cv,\n",
        "#     scoring='accuracy',\n",
        "#     n_jobs=-1,\n",
        "#     random_state=RANDOM_SEED,\n",
        "#     verbose=1\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXFIGYPO_T2H"
      },
      "source": [
        "2. Code Quality\n",
        "\n",
        "Code quality is really important for both **reproducibility** and **teamwork**. One major aspect of maintaining good quality is adding clear **documentation (docstrings)** to all custom functions. Since our code includes several functions like kernels and metrics, adding proper docstrings would make it easier for others (and ourselves later) to understand what each function does and how to use it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riE0FkOJ_dfe"
      },
      "source": [
        "3. Code Optimization:\n",
        "\n",
        "A big part of optimization is figuring out **what** actually needs to be optimized. In our case, the slowest part of the assignment turned out to be the **custom kernels** we implemented.\n",
        "\n",
        "These custom kernels run directly in the Python interpreter, which makes them slower. On the other hand, scikit-learn’s built-in kernels (like `kernel='linear'`) are much faster because they use highly optimized **C and Cython** code under the hood.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAjjb64U_U8o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}